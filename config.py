USE_LOCAL_LLM = False      # Use llama.cpp if True
USE_GPU_LLM = True         # Use GPU transformer model if True
